{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fcfa03e2970>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set random seed \n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n_samples = 50000 # Number of samples in dataset\n",
    "n_continuous = 30 # Number of continuous variables\n",
    "n_categorical = 10 # Number of categorical variables\n",
    "max_categories = 100 # Max number of categories per categorical variable\n",
    "non_informative_prop = 0.3 # Proportion of variables that are non-informative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate continuous features\n",
    "X_continuous = np.random.randn(n_samples, n_continuous)\n",
    "\n",
    "# Generate categorical features\n",
    "X_categorical = []\n",
    "categorical_effects = []\n",
    "for i in range(n_categorical):\n",
    "    n_categories = np.random.randint(2, max_categories)\n",
    "    X_categorical.append(np.random.randint(0, n_categories, n_samples))\n",
    "    if np.random.rand() > non_informative_prop:\n",
    "        categorical_effects.append(np.random.randn(n_categories))\n",
    "    else:\n",
    "        categorical_effects.append(np.zeros(n_categories))\n",
    "X_categorical = np.array(X_categorical).T\n",
    "\n",
    "# Generate target variable\n",
    "y = np.zeros(n_samples)\n",
    "for i in range(n_continuous):\n",
    "    effect = np.random.randn()\n",
    "    y += effect * X_continuous[:, i] + np.random.randn(n_samples) * 0.1\n",
    "\n",
    "for i in range(n_categorical):\n",
    "    y += np.array([categorical_effects[i][cat] for cat in X_categorical[:, i]])\n",
    "\n",
    "y += np.random.randn(n_samples) * 0.5  # Add some noise\n",
    "\n",
    "# Create dataframe\n",
    "df = pd.DataFrame(X_continuous, columns=[f'cont_{i}' for i in range(n_continuous)])\n",
    "for i in range(n_categorical):\n",
    "    df[f'cat_{i}'] = X_categorical[:, i]\n",
    "df['target'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('target', axis=1), df['target'], test_size=0.2, random_state=42)\n",
    "\n",
    "# split test into validation and test\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:41:00] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-cpython-38/xgboost/src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[17:41:00] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-cpython-38/xgboost/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[0]\teval-rmse:6.09420\n",
      "[10]\teval-rmse:5.75561\n",
      "[20]\teval-rmse:5.50345\n",
      "[30]\teval-rmse:5.29281\n",
      "[40]\teval-rmse:5.10784\n",
      "[50]\teval-rmse:4.94518\n",
      "[60]\teval-rmse:4.79726\n",
      "[70]\teval-rmse:4.66008\n",
      "[80]\teval-rmse:4.53761\n",
      "[90]\teval-rmse:4.42476\n",
      "[100]\teval-rmse:4.32184\n",
      "[110]\teval-rmse:4.22671\n",
      "[120]\teval-rmse:4.13877\n",
      "[130]\teval-rmse:4.05566\n",
      "[140]\teval-rmse:3.97783\n",
      "[150]\teval-rmse:3.90740\n",
      "[160]\teval-rmse:3.84162\n",
      "[170]\teval-rmse:3.77844\n",
      "[180]\teval-rmse:3.71859\n",
      "[190]\teval-rmse:3.66355\n",
      "[200]\teval-rmse:3.61191\n",
      "[210]\teval-rmse:3.56438\n",
      "[220]\teval-rmse:3.51808\n",
      "[230]\teval-rmse:3.47068\n",
      "[240]\teval-rmse:3.42917\n",
      "[250]\teval-rmse:3.38972\n",
      "[260]\teval-rmse:3.35296\n",
      "[270]\teval-rmse:3.31353\n",
      "[280]\teval-rmse:3.27751\n",
      "[290]\teval-rmse:3.24167\n",
      "[300]\teval-rmse:3.21088\n",
      "[310]\teval-rmse:3.18154\n",
      "[320]\teval-rmse:3.15095\n",
      "[330]\teval-rmse:3.12167\n",
      "[340]\teval-rmse:3.09520\n",
      "[350]\teval-rmse:3.07196\n",
      "[360]\teval-rmse:3.04579\n",
      "[370]\teval-rmse:3.02278\n",
      "[380]\teval-rmse:3.00125\n",
      "[390]\teval-rmse:2.97941\n",
      "[400]\teval-rmse:2.95617\n",
      "[410]\teval-rmse:2.93223\n",
      "[420]\teval-rmse:2.91156\n",
      "[430]\teval-rmse:2.88755\n",
      "[440]\teval-rmse:2.86809\n",
      "[450]\teval-rmse:2.84940\n",
      "[460]\teval-rmse:2.83260\n",
      "[470]\teval-rmse:2.81450\n",
      "[480]\teval-rmse:2.79514\n",
      "[490]\teval-rmse:2.78036\n",
      "[500]\teval-rmse:2.76303\n",
      "[510]\teval-rmse:2.74583\n",
      "[520]\teval-rmse:2.73257\n",
      "[530]\teval-rmse:2.71447\n",
      "[540]\teval-rmse:2.70229\n",
      "[550]\teval-rmse:2.68560\n",
      "[560]\teval-rmse:2.67072\n",
      "[570]\teval-rmse:2.65733\n",
      "[580]\teval-rmse:2.64532\n",
      "[590]\teval-rmse:2.63376\n",
      "[600]\teval-rmse:2.61951\n",
      "[610]\teval-rmse:2.60873\n",
      "[620]\teval-rmse:2.59003\n",
      "[630]\teval-rmse:2.57994\n",
      "[640]\teval-rmse:2.56761\n",
      "[650]\teval-rmse:2.55902\n",
      "[660]\teval-rmse:2.54692\n",
      "[670]\teval-rmse:2.53469\n",
      "[680]\teval-rmse:2.52611\n",
      "[690]\teval-rmse:2.51649\n",
      "[700]\teval-rmse:2.50840\n",
      "[710]\teval-rmse:2.49240\n",
      "[720]\teval-rmse:2.48321\n",
      "[730]\teval-rmse:2.47690\n",
      "[740]\teval-rmse:2.46441\n",
      "[750]\teval-rmse:2.45516\n",
      "[760]\teval-rmse:2.44628\n",
      "[770]\teval-rmse:2.43617\n",
      "[780]\teval-rmse:2.42239\n",
      "[790]\teval-rmse:2.41211\n",
      "[800]\teval-rmse:2.40077\n",
      "[810]\teval-rmse:2.39160\n",
      "[820]\teval-rmse:2.38028\n",
      "[830]\teval-rmse:2.37292\n",
      "[840]\teval-rmse:2.36481\n",
      "[850]\teval-rmse:2.35248\n",
      "[860]\teval-rmse:2.34066\n",
      "[870]\teval-rmse:2.33175\n",
      "[880]\teval-rmse:2.32340\n",
      "[890]\teval-rmse:2.31457\n",
      "[900]\teval-rmse:2.30437\n",
      "[910]\teval-rmse:2.29271\n",
      "[920]\teval-rmse:2.28374\n",
      "[930]\teval-rmse:2.27739\n",
      "[940]\teval-rmse:2.26729\n",
      "[950]\teval-rmse:2.25942\n",
      "[960]\teval-rmse:2.25345\n",
      "[970]\teval-rmse:2.24467\n",
      "[980]\teval-rmse:2.23772\n",
      "[990]\teval-rmse:2.22651\n",
      "[1000]\teval-rmse:2.21614\n",
      "[1010]\teval-rmse:2.20913\n",
      "[1020]\teval-rmse:2.20353\n",
      "[1030]\teval-rmse:2.19437\n",
      "[1040]\teval-rmse:2.18741\n",
      "[1050]\teval-rmse:2.17830\n",
      "[1060]\teval-rmse:2.16916\n",
      "[1070]\teval-rmse:2.16384\n",
      "[1080]\teval-rmse:2.15515\n",
      "[1090]\teval-rmse:2.14681\n",
      "[1100]\teval-rmse:2.13942\n",
      "[1110]\teval-rmse:2.13428\n",
      "[1120]\teval-rmse:2.12833\n",
      "[1130]\teval-rmse:2.12047\n",
      "[1140]\teval-rmse:2.11537\n",
      "[1150]\teval-rmse:2.10935\n",
      "[1160]\teval-rmse:2.10306\n",
      "[1170]\teval-rmse:2.09553\n",
      "[1180]\teval-rmse:2.08515\n",
      "[1190]\teval-rmse:2.07500\n",
      "[1200]\teval-rmse:2.06855\n",
      "[1210]\teval-rmse:2.05937\n",
      "[1220]\teval-rmse:2.05256\n",
      "[1230]\teval-rmse:2.04595\n",
      "[1240]\teval-rmse:2.03382\n",
      "[1250]\teval-rmse:2.02817\n",
      "[1260]\teval-rmse:2.01943\n",
      "[1270]\teval-rmse:2.01403\n",
      "[1280]\teval-rmse:2.00464\n",
      "[1290]\teval-rmse:1.99352\n",
      "[1300]\teval-rmse:1.98737\n",
      "[1310]\teval-rmse:1.98016\n",
      "[1320]\teval-rmse:1.97541\n",
      "[1330]\teval-rmse:1.96905\n",
      "[1340]\teval-rmse:1.96348\n",
      "[1350]\teval-rmse:1.95820\n",
      "[1360]\teval-rmse:1.94865\n",
      "[1370]\teval-rmse:1.94357\n",
      "[1380]\teval-rmse:1.93779\n",
      "[1390]\teval-rmse:1.92916\n",
      "[1400]\teval-rmse:1.92460\n",
      "[1410]\teval-rmse:1.91520\n",
      "[1420]\teval-rmse:1.90684\n",
      "[1430]\teval-rmse:1.90128\n",
      "[1440]\teval-rmse:1.89502\n",
      "[1450]\teval-rmse:1.88840\n",
      "[1460]\teval-rmse:1.88302\n",
      "[1470]\teval-rmse:1.87539\n",
      "[1480]\teval-rmse:1.86403\n",
      "[1490]\teval-rmse:1.86155\n",
      "[1500]\teval-rmse:1.85258\n",
      "[1510]\teval-rmse:1.84740\n",
      "[1520]\teval-rmse:1.84358\n",
      "[1530]\teval-rmse:1.83740\n",
      "[1540]\teval-rmse:1.83184\n",
      "[1550]\teval-rmse:1.82685\n",
      "[1560]\teval-rmse:1.81880\n",
      "[1570]\teval-rmse:1.81193\n",
      "[1580]\teval-rmse:1.80523\n",
      "[1590]\teval-rmse:1.80187\n",
      "[1600]\teval-rmse:1.79425\n",
      "[1610]\teval-rmse:1.78916\n",
      "[1620]\teval-rmse:1.78214\n",
      "[1630]\teval-rmse:1.77535\n",
      "[1640]\teval-rmse:1.77036\n",
      "[1650]\teval-rmse:1.76539\n",
      "[1660]\teval-rmse:1.75955\n",
      "[1670]\teval-rmse:1.75434\n",
      "[1680]\teval-rmse:1.75071\n",
      "[1690]\teval-rmse:1.74583\n",
      "[1700]\teval-rmse:1.73950\n",
      "[1710]\teval-rmse:1.73248\n",
      "[1720]\teval-rmse:1.72855\n",
      "[1730]\teval-rmse:1.72318\n",
      "[1740]\teval-rmse:1.71813\n",
      "[1750]\teval-rmse:1.71214\n",
      "[1760]\teval-rmse:1.70650\n",
      "[1770]\teval-rmse:1.70258\n",
      "[1780]\teval-rmse:1.69839\n",
      "[1790]\teval-rmse:1.69506\n",
      "[1800]\teval-rmse:1.69132\n",
      "[1810]\teval-rmse:1.68748\n",
      "[1820]\teval-rmse:1.68594\n",
      "[1830]\teval-rmse:1.68186\n",
      "[1840]\teval-rmse:1.67488\n",
      "[1850]\teval-rmse:1.67150\n",
      "[1860]\teval-rmse:1.66446\n",
      "[1870]\teval-rmse:1.66150\n",
      "[1880]\teval-rmse:1.65821\n",
      "[1890]\teval-rmse:1.65398\n",
      "[1900]\teval-rmse:1.64962\n",
      "[1910]\teval-rmse:1.64598\n",
      "[1920]\teval-rmse:1.64160\n",
      "[1930]\teval-rmse:1.63707\n",
      "[1940]\teval-rmse:1.63019\n",
      "[1950]\teval-rmse:1.62556\n",
      "[1960]\teval-rmse:1.62108\n",
      "[1970]\teval-rmse:1.61315\n",
      "[1980]\teval-rmse:1.60970\n",
      "[1990]\teval-rmse:1.60486\n",
      "[2000]\teval-rmse:1.59915\n",
      "[2010]\teval-rmse:1.59548\n",
      "[2020]\teval-rmse:1.59057\n",
      "[2030]\teval-rmse:1.58589\n",
      "[2040]\teval-rmse:1.58269\n",
      "[2050]\teval-rmse:1.58024\n",
      "[2060]\teval-rmse:1.57760\n",
      "[2070]\teval-rmse:1.57419\n",
      "[2080]\teval-rmse:1.56981\n",
      "[2090]\teval-rmse:1.56556\n",
      "[2100]\teval-rmse:1.56043\n",
      "[2110]\teval-rmse:1.55699\n",
      "[2120]\teval-rmse:1.55215\n",
      "[2130]\teval-rmse:1.54762\n",
      "[2140]\teval-rmse:1.54306\n",
      "[2150]\teval-rmse:1.53870\n",
      "[2160]\teval-rmse:1.53610\n",
      "[2170]\teval-rmse:1.53271\n",
      "[2180]\teval-rmse:1.52921\n",
      "[2190]\teval-rmse:1.52566\n",
      "[2200]\teval-rmse:1.52033\n",
      "[2210]\teval-rmse:1.51699\n",
      "[2220]\teval-rmse:1.51181\n",
      "[2230]\teval-rmse:1.50818\n",
      "[2240]\teval-rmse:1.50591\n",
      "[2250]\teval-rmse:1.50304\n",
      "[2260]\teval-rmse:1.49873\n",
      "[2270]\teval-rmse:1.49457\n",
      "[2280]\teval-rmse:1.49157\n",
      "[2290]\teval-rmse:1.48872\n",
      "[2300]\teval-rmse:1.48604\n",
      "[2310]\teval-rmse:1.48372\n",
      "[2320]\teval-rmse:1.48094\n",
      "[2330]\teval-rmse:1.47615\n",
      "[2340]\teval-rmse:1.47332\n",
      "[2350]\teval-rmse:1.47035\n",
      "[2360]\teval-rmse:1.46815\n",
      "[2370]\teval-rmse:1.46187\n",
      "[2380]\teval-rmse:1.45860\n",
      "[2390]\teval-rmse:1.45603\n",
      "[2400]\teval-rmse:1.45309\n",
      "[2410]\teval-rmse:1.44992\n",
      "[2420]\teval-rmse:1.44644\n",
      "[2430]\teval-rmse:1.44325\n",
      "[2440]\teval-rmse:1.43868\n",
      "[2450]\teval-rmse:1.43537\n",
      "[2460]\teval-rmse:1.43265\n",
      "[2470]\teval-rmse:1.42979\n",
      "[2480]\teval-rmse:1.42620\n",
      "[2490]\teval-rmse:1.42306\n",
      "[2500]\teval-rmse:1.41895\n",
      "[2510]\teval-rmse:1.41627\n",
      "[2520]\teval-rmse:1.41348\n",
      "[2530]\teval-rmse:1.41098\n",
      "[2540]\teval-rmse:1.40784\n",
      "[2550]\teval-rmse:1.40514\n",
      "[2560]\teval-rmse:1.40291\n",
      "[2570]\teval-rmse:1.40042\n",
      "[2580]\teval-rmse:1.39828\n",
      "[2590]\teval-rmse:1.39585\n",
      "[2600]\teval-rmse:1.39237\n",
      "[2610]\teval-rmse:1.38992\n",
      "[2620]\teval-rmse:1.38710\n",
      "[2630]\teval-rmse:1.38436\n",
      "[2640]\teval-rmse:1.38167\n",
      "[2650]\teval-rmse:1.37907\n",
      "[2660]\teval-rmse:1.37723\n",
      "[2670]\teval-rmse:1.37522\n",
      "[2680]\teval-rmse:1.37259\n",
      "[2690]\teval-rmse:1.37077\n",
      "[2700]\teval-rmse:1.36899\n",
      "[2710]\teval-rmse:1.36681\n",
      "[2720]\teval-rmse:1.36359\n",
      "[2730]\teval-rmse:1.36140\n",
      "[2740]\teval-rmse:1.35920\n",
      "[2750]\teval-rmse:1.35685\n",
      "[2760]\teval-rmse:1.35431\n",
      "[2770]\teval-rmse:1.35237\n",
      "[2780]\teval-rmse:1.35027\n",
      "[2790]\teval-rmse:1.34825\n",
      "[2800]\teval-rmse:1.34667\n",
      "[2810]\teval-rmse:1.34385\n",
      "[2820]\teval-rmse:1.34170\n",
      "[2830]\teval-rmse:1.34042\n",
      "[2840]\teval-rmse:1.33897\n",
      "[2850]\teval-rmse:1.33619\n",
      "[2860]\teval-rmse:1.33412\n",
      "[2870]\teval-rmse:1.33163\n",
      "[2880]\teval-rmse:1.32978\n",
      "[2890]\teval-rmse:1.32683\n",
      "[2900]\teval-rmse:1.32423\n",
      "[2910]\teval-rmse:1.32214\n",
      "[2920]\teval-rmse:1.31962\n",
      "[2930]\teval-rmse:1.31767\n",
      "[2940]\teval-rmse:1.31559\n",
      "[2950]\teval-rmse:1.31407\n",
      "[2960]\teval-rmse:1.31160\n",
      "[2970]\teval-rmse:1.30953\n",
      "[2980]\teval-rmse:1.30801\n",
      "[2990]\teval-rmse:1.30640\n",
      "[3000]\teval-rmse:1.30400\n",
      "[3010]\teval-rmse:1.30241\n",
      "[3020]\teval-rmse:1.30124\n",
      "[3030]\teval-rmse:1.29964\n",
      "[3040]\teval-rmse:1.29737\n",
      "[3050]\teval-rmse:1.29654\n",
      "[3060]\teval-rmse:1.29482\n",
      "[3070]\teval-rmse:1.29301\n",
      "[3080]\teval-rmse:1.29100\n",
      "[3090]\teval-rmse:1.28912\n",
      "[3100]\teval-rmse:1.28823\n",
      "[3110]\teval-rmse:1.28669\n",
      "[3120]\teval-rmse:1.28473\n",
      "[3130]\teval-rmse:1.28248\n",
      "[3140]\teval-rmse:1.28102\n",
      "[3150]\teval-rmse:1.27976\n",
      "[3160]\teval-rmse:1.27840\n",
      "[3170]\teval-rmse:1.27632\n",
      "[3180]\teval-rmse:1.27397\n",
      "[3190]\teval-rmse:1.27221\n",
      "[3200]\teval-rmse:1.27091\n",
      "[3210]\teval-rmse:1.26863\n",
      "[3220]\teval-rmse:1.26741\n",
      "[3230]\teval-rmse:1.26548\n",
      "[3240]\teval-rmse:1.26378\n",
      "[3250]\teval-rmse:1.26197\n",
      "[3260]\teval-rmse:1.26027\n",
      "[3270]\teval-rmse:1.25842\n",
      "[3280]\teval-rmse:1.25680\n",
      "[3290]\teval-rmse:1.25544\n",
      "[3300]\teval-rmse:1.25434\n",
      "[3310]\teval-rmse:1.25259\n",
      "[3320]\teval-rmse:1.25098\n",
      "[3330]\teval-rmse:1.24960\n",
      "[3340]\teval-rmse:1.24810\n",
      "[3350]\teval-rmse:1.24676\n",
      "[3360]\teval-rmse:1.24518\n",
      "[3370]\teval-rmse:1.24425\n",
      "[3380]\teval-rmse:1.24327\n",
      "[3390]\teval-rmse:1.24196\n",
      "[3400]\teval-rmse:1.24029\n",
      "[3410]\teval-rmse:1.23855\n",
      "[3420]\teval-rmse:1.23757\n",
      "[3430]\teval-rmse:1.23686\n",
      "[3440]\teval-rmse:1.23512\n",
      "[3450]\teval-rmse:1.23396\n",
      "[3460]\teval-rmse:1.23247\n",
      "[3470]\teval-rmse:1.23143\n",
      "[3480]\teval-rmse:1.23051\n",
      "[3490]\teval-rmse:1.22937\n",
      "[3500]\teval-rmse:1.22769\n",
      "[3510]\teval-rmse:1.22559\n",
      "[3520]\teval-rmse:1.22350\n",
      "[3530]\teval-rmse:1.22271\n",
      "[3540]\teval-rmse:1.22210\n",
      "[3550]\teval-rmse:1.22071\n",
      "[3560]\teval-rmse:1.21972\n",
      "[3570]\teval-rmse:1.21891\n",
      "[3580]\teval-rmse:1.21795\n",
      "[3590]\teval-rmse:1.21652\n",
      "[3600]\teval-rmse:1.21528\n",
      "[3610]\teval-rmse:1.21452\n",
      "[3620]\teval-rmse:1.21311\n",
      "[3630]\teval-rmse:1.21183\n",
      "[3640]\teval-rmse:1.20993\n",
      "[3650]\teval-rmse:1.20885\n",
      "[3660]\teval-rmse:1.20757\n",
      "[3670]\teval-rmse:1.20653\n",
      "[3680]\teval-rmse:1.20518\n",
      "[3690]\teval-rmse:1.20411\n",
      "[3700]\teval-rmse:1.20278\n",
      "[3710]\teval-rmse:1.20141\n",
      "[3720]\teval-rmse:1.20003\n",
      "[3730]\teval-rmse:1.19905\n",
      "[3740]\teval-rmse:1.19843\n",
      "[3750]\teval-rmse:1.19771\n",
      "[3760]\teval-rmse:1.19686\n",
      "[3770]\teval-rmse:1.19633\n",
      "[3780]\teval-rmse:1.19510\n",
      "[3790]\teval-rmse:1.19403\n",
      "[3800]\teval-rmse:1.19263\n",
      "[3810]\teval-rmse:1.19209\n",
      "[3820]\teval-rmse:1.19112\n",
      "[3830]\teval-rmse:1.18999\n",
      "[3840]\teval-rmse:1.18948\n",
      "[3850]\teval-rmse:1.18833\n",
      "[3860]\teval-rmse:1.18740\n",
      "[3870]\teval-rmse:1.18728\n",
      "[3880]\teval-rmse:1.18534\n",
      "[3890]\teval-rmse:1.18469\n",
      "[3900]\teval-rmse:1.18369\n",
      "[3910]\teval-rmse:1.18281\n",
      "[3920]\teval-rmse:1.18155\n",
      "[3930]\teval-rmse:1.18077\n",
      "[3940]\teval-rmse:1.17995\n",
      "[3950]\teval-rmse:1.17915\n",
      "[3960]\teval-rmse:1.17801\n",
      "[3970]\teval-rmse:1.17726\n",
      "[3980]\teval-rmse:1.17630\n",
      "[3990]\teval-rmse:1.17568\n",
      "[4000]\teval-rmse:1.17466\n",
      "[4010]\teval-rmse:1.17377\n",
      "[4020]\teval-rmse:1.17287\n",
      "[4030]\teval-rmse:1.17185\n",
      "[4040]\teval-rmse:1.17098\n",
      "[4050]\teval-rmse:1.16994\n",
      "[4060]\teval-rmse:1.16947\n",
      "[4070]\teval-rmse:1.16892\n",
      "[4080]\teval-rmse:1.16774\n",
      "[4090]\teval-rmse:1.16683\n",
      "[4100]\teval-rmse:1.16600\n",
      "[4110]\teval-rmse:1.16549\n",
      "[4120]\teval-rmse:1.16416\n",
      "[4130]\teval-rmse:1.16288\n",
      "[4140]\teval-rmse:1.16145\n",
      "[4150]\teval-rmse:1.16058\n",
      "[4160]\teval-rmse:1.15983\n",
      "[4170]\teval-rmse:1.15931\n",
      "[4180]\teval-rmse:1.15865\n",
      "[4190]\teval-rmse:1.15815\n",
      "[4200]\teval-rmse:1.15740\n",
      "[4210]\teval-rmse:1.15653\n",
      "[4220]\teval-rmse:1.15586\n",
      "[4230]\teval-rmse:1.15477\n",
      "[4240]\teval-rmse:1.15425\n",
      "[4250]\teval-rmse:1.15310\n",
      "[4260]\teval-rmse:1.15266\n",
      "[4270]\teval-rmse:1.15200\n",
      "[4280]\teval-rmse:1.15133\n",
      "[4290]\teval-rmse:1.15062\n",
      "[4300]\teval-rmse:1.14991\n",
      "[4310]\teval-rmse:1.14933\n",
      "[4320]\teval-rmse:1.14838\n",
      "[4330]\teval-rmse:1.14757\n",
      "[4340]\teval-rmse:1.14713\n",
      "[4350]\teval-rmse:1.14652\n",
      "[4360]\teval-rmse:1.14622\n",
      "[4370]\teval-rmse:1.14597\n",
      "[4380]\teval-rmse:1.14519\n",
      "[4390]\teval-rmse:1.14439\n",
      "[4400]\teval-rmse:1.14371\n",
      "[4410]\teval-rmse:1.14279\n",
      "[4420]\teval-rmse:1.14181\n",
      "[4430]\teval-rmse:1.14147\n",
      "[4440]\teval-rmse:1.14087\n",
      "[4450]\teval-rmse:1.14069\n",
      "[4460]\teval-rmse:1.14015\n",
      "[4470]\teval-rmse:1.13944\n",
      "[4480]\teval-rmse:1.13911\n",
      "[4490]\teval-rmse:1.13854\n",
      "[4500]\teval-rmse:1.13802\n",
      "[4510]\teval-rmse:1.13756\n",
      "[4520]\teval-rmse:1.13656\n",
      "[4530]\teval-rmse:1.13558\n",
      "[4540]\teval-rmse:1.13477\n",
      "[4550]\teval-rmse:1.13421\n",
      "[4560]\teval-rmse:1.13384\n",
      "[4570]\teval-rmse:1.13301\n",
      "[4580]\teval-rmse:1.13237\n",
      "[4590]\teval-rmse:1.13225\n",
      "[4600]\teval-rmse:1.13171\n",
      "[4610]\teval-rmse:1.13151\n",
      "[4620]\teval-rmse:1.13048\n",
      "[4630]\teval-rmse:1.12999\n",
      "[4640]\teval-rmse:1.12895\n",
      "[4650]\teval-rmse:1.12838\n",
      "[4660]\teval-rmse:1.12818\n",
      "[4670]\teval-rmse:1.12732\n",
      "[4680]\teval-rmse:1.12649\n",
      "[4690]\teval-rmse:1.12612\n",
      "[4700]\teval-rmse:1.12528\n",
      "[4710]\teval-rmse:1.12473\n",
      "[4720]\teval-rmse:1.12431\n",
      "[4730]\teval-rmse:1.12412\n",
      "[4740]\teval-rmse:1.12326\n",
      "[4750]\teval-rmse:1.12288\n",
      "[4760]\teval-rmse:1.12227\n",
      "[4770]\teval-rmse:1.12193\n",
      "[4780]\teval-rmse:1.12150\n",
      "[4790]\teval-rmse:1.12086\n",
      "[4800]\teval-rmse:1.12028\n",
      "[4810]\teval-rmse:1.11967\n",
      "[4820]\teval-rmse:1.11917\n",
      "[4830]\teval-rmse:1.11885\n",
      "[4840]\teval-rmse:1.11858\n",
      "[4850]\teval-rmse:1.11800\n",
      "[4860]\teval-rmse:1.11731\n",
      "[4870]\teval-rmse:1.11642\n",
      "[4880]\teval-rmse:1.11624\n",
      "[4890]\teval-rmse:1.11596\n",
      "[4900]\teval-rmse:1.11540\n",
      "[4910]\teval-rmse:1.11483\n",
      "[4920]\teval-rmse:1.11439\n",
      "[4930]\teval-rmse:1.11408\n",
      "[4940]\teval-rmse:1.11344\n",
      "[4950]\teval-rmse:1.11304\n",
      "[4960]\teval-rmse:1.11281\n",
      "[4970]\teval-rmse:1.11254\n",
      "[4980]\teval-rmse:1.11241\n",
      "[4990]\teval-rmse:1.11193\n",
      "[5000]\teval-rmse:1.11139\n",
      "[5010]\teval-rmse:1.11102\n",
      "[5020]\teval-rmse:1.11020\n",
      "[5030]\teval-rmse:1.10972\n",
      "[5040]\teval-rmse:1.10931\n",
      "[5050]\teval-rmse:1.10890\n",
      "[5060]\teval-rmse:1.10875\n",
      "[5070]\teval-rmse:1.10846\n",
      "[5080]\teval-rmse:1.10803\n",
      "[5090]\teval-rmse:1.10784\n",
      "[5100]\teval-rmse:1.10747\n",
      "[5110]\teval-rmse:1.10716\n",
      "[5120]\teval-rmse:1.10682\n",
      "[5130]\teval-rmse:1.10616\n",
      "[5140]\teval-rmse:1.10591\n",
      "[5150]\teval-rmse:1.10534\n",
      "[5160]\teval-rmse:1.10489\n",
      "[5170]\teval-rmse:1.10474\n",
      "[5180]\teval-rmse:1.10442\n",
      "[5190]\teval-rmse:1.10396\n",
      "[5200]\teval-rmse:1.10345\n",
      "[5210]\teval-rmse:1.10313\n",
      "[5220]\teval-rmse:1.10284\n",
      "[5230]\teval-rmse:1.10265\n",
      "[5240]\teval-rmse:1.10218\n",
      "[5250]\teval-rmse:1.10192\n",
      "[5260]\teval-rmse:1.10164\n",
      "[5270]\teval-rmse:1.10133\n",
      "[5280]\teval-rmse:1.10112\n",
      "[5290]\teval-rmse:1.10087\n",
      "[5300]\teval-rmse:1.10068\n",
      "[5310]\teval-rmse:1.10000\n",
      "[5320]\teval-rmse:1.09958\n",
      "[5330]\teval-rmse:1.09855\n",
      "[5340]\teval-rmse:1.09832\n",
      "[5350]\teval-rmse:1.09787\n",
      "[5360]\teval-rmse:1.09763\n",
      "[5366]\teval-rmse:1.09764\n",
      "XGBoost MSE: 1.2302821228747942\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# split the data into training and testing sets\n",
    "\n",
    "# create an xgboost matrix\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dval = xgb.DMatrix(X_val, label=y_val)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# set the parameters for the xgboost model\n",
    "param = {\n",
    "    'max_depth': 3,  # the maximum depth of each tree\n",
    "    'eta': 0.05,  # the training step for each iteration\n",
    "    'silent': 1,  # logging mode - quiet\n",
    "    'objective': 'reg:linear',  # error evaluation for multiclass training # early stopping rounds\n",
    "    'eval_metric': 'rmse',\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'seed': 0,\n",
    "    'nthread': 8,\n",
    "}\n",
    "\n",
    "# specify validations set to watch performance\n",
    "watchlist = [(dval, 'eval'), (dtrain, 'train')]\n",
    "\n",
    "# train the model\n",
    "num_round = 100000\n",
    "xgb_model = xgb.train(param, dtrain, num_round, evals=[(dval, 'eval')], early_stopping_rounds=10, verbose_eval=10)\n",
    "\n",
    "xgb_pred = xgb_model.predict(dtest)\n",
    "xgb_mse = mean_squared_error(y_test, xgb_pred)\n",
    "print(f\"XGBoost MSE: {xgb_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:42:59] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-cpython-38/xgboost/src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[17:42:59] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-cpython-38/xgboost/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[0]\teval-rmse:6.09711\n",
      "[10]\teval-rmse:5.75215\n",
      "[20]\teval-rmse:5.50318\n",
      "[30]\teval-rmse:5.29259\n",
      "[40]\teval-rmse:5.11205\n",
      "[50]\teval-rmse:4.95017\n",
      "[60]\teval-rmse:4.79739\n",
      "[70]\teval-rmse:4.66263\n",
      "[80]\teval-rmse:4.54233\n",
      "[90]\teval-rmse:4.43121\n",
      "[100]\teval-rmse:4.32842\n",
      "[110]\teval-rmse:4.23703\n",
      "[120]\teval-rmse:4.15107\n",
      "[130]\teval-rmse:4.07188\n",
      "[140]\teval-rmse:4.00003\n",
      "[150]\teval-rmse:3.93037\n",
      "[160]\teval-rmse:3.86903\n",
      "[170]\teval-rmse:3.80709\n",
      "[180]\teval-rmse:3.75230\n",
      "[190]\teval-rmse:3.69926\n",
      "[200]\teval-rmse:3.64930\n",
      "[210]\teval-rmse:3.60380\n",
      "[220]\teval-rmse:3.56108\n",
      "[230]\teval-rmse:3.52028\n",
      "[240]\teval-rmse:3.48033\n",
      "[250]\teval-rmse:3.44360\n",
      "[260]\teval-rmse:3.40655\n",
      "[270]\teval-rmse:3.37603\n",
      "[280]\teval-rmse:3.34508\n",
      "[290]\teval-rmse:3.31604\n",
      "[300]\teval-rmse:3.28788\n",
      "[310]\teval-rmse:3.26014\n",
      "[320]\teval-rmse:3.23337\n",
      "[330]\teval-rmse:3.21052\n",
      "[340]\teval-rmse:3.18924\n",
      "[350]\teval-rmse:3.16842\n",
      "[360]\teval-rmse:3.14890\n",
      "[370]\teval-rmse:3.13015\n",
      "[380]\teval-rmse:3.11346\n",
      "[390]\teval-rmse:3.09380\n",
      "[400]\teval-rmse:3.07792\n",
      "[410]\teval-rmse:3.06023\n",
      "[420]\teval-rmse:3.04595\n",
      "[430]\teval-rmse:3.03022\n",
      "[440]\teval-rmse:3.01756\n",
      "[450]\teval-rmse:3.00566\n",
      "[460]\teval-rmse:2.99239\n",
      "[470]\teval-rmse:2.97996\n",
      "[480]\teval-rmse:2.96788\n",
      "[490]\teval-rmse:2.95786\n",
      "[500]\teval-rmse:2.94862\n",
      "[510]\teval-rmse:2.93936\n",
      "[520]\teval-rmse:2.92987\n",
      "[530]\teval-rmse:2.91995\n",
      "[540]\teval-rmse:2.91180\n",
      "[550]\teval-rmse:2.90358\n",
      "[560]\teval-rmse:2.89543\n",
      "[570]\teval-rmse:2.88776\n",
      "[580]\teval-rmse:2.88076\n",
      "[590]\teval-rmse:2.87396\n",
      "[600]\teval-rmse:2.86772\n",
      "[610]\teval-rmse:2.86127\n",
      "[620]\teval-rmse:2.85516\n",
      "[630]\teval-rmse:2.84937\n",
      "[640]\teval-rmse:2.84439\n",
      "[650]\teval-rmse:2.83920\n",
      "[660]\teval-rmse:2.83389\n",
      "[670]\teval-rmse:2.82888\n",
      "[680]\teval-rmse:2.82437\n",
      "[690]\teval-rmse:2.82015\n",
      "[700]\teval-rmse:2.81542\n",
      "[710]\teval-rmse:2.81122\n",
      "[720]\teval-rmse:2.80766\n",
      "[730]\teval-rmse:2.80414\n",
      "[740]\teval-rmse:2.80049\n",
      "[750]\teval-rmse:2.79677\n",
      "[760]\teval-rmse:2.79335\n",
      "[770]\teval-rmse:2.79043\n",
      "[780]\teval-rmse:2.78706\n",
      "[790]\teval-rmse:2.78450\n",
      "[800]\teval-rmse:2.78190\n",
      "[810]\teval-rmse:2.77871\n",
      "[820]\teval-rmse:2.77668\n",
      "[830]\teval-rmse:2.77413\n",
      "[840]\teval-rmse:2.77222\n",
      "[850]\teval-rmse:2.77010\n",
      "[860]\teval-rmse:2.76776\n",
      "[870]\teval-rmse:2.76458\n",
      "[880]\teval-rmse:2.76216\n",
      "[890]\teval-rmse:2.76039\n",
      "[900]\teval-rmse:2.75905\n",
      "[910]\teval-rmse:2.75803\n",
      "[920]\teval-rmse:2.75634\n",
      "[930]\teval-rmse:2.75482\n",
      "[940]\teval-rmse:2.75328\n",
      "[950]\teval-rmse:2.75219\n",
      "[960]\teval-rmse:2.75063\n",
      "[970]\teval-rmse:2.74942\n",
      "[980]\teval-rmse:2.74804\n",
      "[990]\teval-rmse:2.74656\n",
      "[1000]\teval-rmse:2.74534\n",
      "[1010]\teval-rmse:2.74397\n",
      "[1020]\teval-rmse:2.74287\n",
      "[1030]\teval-rmse:2.74204\n",
      "[1040]\teval-rmse:2.74109\n",
      "[1050]\teval-rmse:2.74031\n",
      "[1060]\teval-rmse:2.73925\n",
      "[1070]\teval-rmse:2.73840\n",
      "[1080]\teval-rmse:2.73744\n",
      "[1090]\teval-rmse:2.73625\n",
      "[1100]\teval-rmse:2.73508\n",
      "[1110]\teval-rmse:2.73441\n",
      "[1120]\teval-rmse:2.73338\n",
      "[1130]\teval-rmse:2.73249\n",
      "[1140]\teval-rmse:2.73212\n",
      "[1150]\teval-rmse:2.73141\n",
      "[1160]\teval-rmse:2.73053\n",
      "[1170]\teval-rmse:2.73003\n",
      "[1180]\teval-rmse:2.72905\n",
      "[1190]\teval-rmse:2.72867\n",
      "[1200]\teval-rmse:2.72821\n",
      "[1210]\teval-rmse:2.72736\n",
      "[1220]\teval-rmse:2.72690\n",
      "[1230]\teval-rmse:2.72647\n",
      "[1240]\teval-rmse:2.72635\n",
      "[1250]\teval-rmse:2.72566\n",
      "[1260]\teval-rmse:2.72502\n",
      "[1270]\teval-rmse:2.72480\n",
      "[1280]\teval-rmse:2.72446\n",
      "[1290]\teval-rmse:2.72403\n",
      "[1300]\teval-rmse:2.72393\n",
      "[1310]\teval-rmse:2.72346\n",
      "[1320]\teval-rmse:2.72325\n",
      "[1330]\teval-rmse:2.72343\n",
      "[1336]\teval-rmse:2.72349\n"
     ]
    }
   ],
   "source": [
    "# Stacked model (GBM + NN)\n",
    "\n",
    "# First, train a GBM on continuous features\n",
    "cont_cols = [col for col in X_train.columns if col.startswith('cont_')]\n",
    "cat_cols = [col for col in X_train.columns if col.startswith('cat_')]\n",
    "\n",
    "\n",
    "\n",
    "# Split the data\n",
    "X_train_cat, X_val_cat, X_test_cat = X_train[cat_cols], X_val[cat_cols], X_test[cat_cols]\n",
    "X_train, X_val, X_test = X_train[cont_cols], X_val[cont_cols], X_test[cont_cols]\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dval = xgb.DMatrix(X_val, label=y_val)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# set the parameters for the xgboost model\n",
    "param = {\n",
    "    'max_depth': 3,  # the maximum depth of each tree\n",
    "    'eta': 0.05,  # the training step for each iteration\n",
    "    'silent': 1,  # logging mode - quiet\n",
    "    'objective': 'reg:linear',  # error evaluation for multiclass training # early stopping rounds\n",
    "    'eval_metric': 'rmse',\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'seed': 0,\n",
    "    'nthread': 8,\n",
    "}\n",
    "\n",
    "# specify validations set to watch performance\n",
    "watchlist = [(dval, 'eval'), (dtrain, 'train')]\n",
    "\n",
    "# train the model\n",
    "num_round = 100000\n",
    "xgb_model = xgb.train(param, dtrain, num_round, evals=[(dval, 'eval')], early_stopping_rounds=10, verbose_eval=10)\n",
    "\n",
    "gbm_train_pred = xgb_model.predict(dtrain)\n",
    "gbm_test_pred = xgb_model.predict(dtest)\n",
    "\n",
    "# Prepare data for neural network\n",
    "label_encoders = {}\n",
    "X_train_cat_encoded = np.zeros_like(X_train_cat)\n",
    "X_test_cat_encoded = np.zeros_like(X_test_cat)\n",
    "\n",
    "for i, col in enumerate(cat_cols):\n",
    "    le = LabelEncoder()\n",
    "    X_train_cat_encoded[:, i] = le.fit_transform(X_train_cat[col])\n",
    "    X_test_cat_encoded[:, i] = le.transform(X_test_cat[col])\n",
    "    label_encoders[col] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Dataset\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, cat_data, cont_data, gbm_pred, targets):\n",
    "        self.cat_data = torch.tensor(cat_data, dtype=torch.long)\n",
    "        self.cont_data = torch.tensor(cont_data, dtype=torch.float32)\n",
    "        self.gbm_pred = torch.tensor(gbm_pred, dtype=torch.float32)\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.cat_data[idx], self.cont_data[idx], self.gbm_pred[idx], self.targets[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.294677855618545\n",
      "0\n",
      "1.0477133014698317\n",
      "1\n",
      "1.0171995198647494\n",
      "2\n",
      "0.9199809940965704\n",
      "3\n",
      "0.8523584147263394\n",
      "4\n",
      "0.7992217637240148\n",
      "5\n",
      "0.7840515023619308\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "train_dataset = TabularDataset(X_train_cat_encoded, X_train[cont_cols].values, gbm_train_pred, y_train.values)\n",
    "test_dataset = TabularDataset(X_test_cat_encoded, X_test[cont_cols].values, gbm_test_pred, y_test.values)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Neural Network model\n",
    "class StackedModel(nn.Module):\n",
    "    def __init__(self, cat_dims, embed_dims, cont_dim):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(dim, embed_dim) \n",
    "                                         for dim, embed_dim in zip(cat_dims, embed_dims)])\n",
    "        self.num_embeddings = sum(embed_dims)\n",
    "        self.fc1 = nn.Linear(self.num_embeddings + cont_dim + 1, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, cat_input, cont_input, gbm_pred):\n",
    "        embedded = [emb(cat_input[:, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        embedded = torch.cat(embedded, dim=1)\n",
    "        x = torch.cat([embedded, cont_input, gbm_pred.unsqueeze(1)], dim=1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Instantiate the model\n",
    "cat_dims = [len(le.classes_) for le in label_encoders.values()]\n",
    "embed_dims = [min(50, (dim + 1) // 2) for dim in cat_dims]\n",
    "model = StackedModel(cat_dims, embed_dims, len(cont_cols))\n",
    "\n",
    "# Training loop\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "num_epochs = 10\n",
    "best_score = 100000\n",
    "best_model = None\n",
    "for epoch in range(num_epochs):\n",
    "    model.eval()\n",
    "    stacked_preds = []\n",
    "    with torch.no_grad():\n",
    "        for cat_data, cont_data, gbm_pred, _ in test_loader:\n",
    "            outputs = model(cat_data, cont_data, gbm_pred)\n",
    "            stacked_preds.append(outputs.numpy())\n",
    "\n",
    "    stacked_preds = np.concatenate(stacked_preds).flatten()\n",
    "    stacked_mse = mean_squared_error(y_test, stacked_preds)\n",
    "    if stacked_mse < best_score:\n",
    "        best_score = stacked_mse\n",
    "        best_model = model # need to figure out how to copy\n",
    "        print(stacked_mse)\n",
    "    else:\n",
    "        break\n",
    "    print(epoch)\n",
    "    model.train()\n",
    "    for cat_data, cont_data, gbm_pred, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = None\n",
    "        try:\n",
    "            outputs = model(cat_data, cont_data, gbm_pred)\n",
    "        except:\n",
    "            outputs = model(cat_data, cont_data, gbn_pred.unsqueeze(1))\n",
    "        loss = criterion(outputs, targets.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XBG MSE: 1.2302821228747942\n",
      "Stacked Model MSE: 0.8187297941387333\n",
      "Improvement: 33.45%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "stacked_preds = []\n",
    "with torch.no_grad():\n",
    "    for cat_data, cont_data, gbm_pred, _ in test_loader:\n",
    "        outputs = model(cat_data, cont_data, gbm_pred)\n",
    "        stacked_preds.append(outputs.numpy())\n",
    "\n",
    "stacked_preds = np.concatenate(stacked_preds).flatten()\n",
    "stacked_mse = mean_squared_error(y_test, stacked_preds)\n",
    "\n",
    "print(\"XBG MSE:\", xgb_mse)\n",
    "print(f\"Stacked Model MSE: {stacked_mse}\")\n",
    "\n",
    "print(f\"Improvement: {(xgb_mse - stacked_mse) / xgb_mse * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
